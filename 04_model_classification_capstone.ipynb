{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Classification Model <br>\n",
    "\n",
    "#### Brandie Hatch\n",
    "\n",
    "### Modeling\n",
    "\n",
    "- Imports, Data Reading, Model Prep\n",
    "- Modeling\n",
    "- Model Evaluation\n",
    "\n",
    "#### Modeling:\n",
    "Supervised Classification - likely a combination of Logistic Regression, Random Forest, Decision Trees, Naive Bayes, and SVM to see which one performs the best at accurately predicting male or female based on education attainment, labor types, income, etc. Then, use that type of model to get the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Data Reading, and Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, ConfusionMatrixDisplay, plot_roc_curve, roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC, SVC \n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns =999\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3239553, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>COW</th>\n",
       "      <th>ENG</th>\n",
       "      <th>ESR</th>\n",
       "      <th>FFSP</th>\n",
       "      <th>LANX</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MULTG</th>\n",
       "      <th>NOC</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>POVPIP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>SCH</th>\n",
       "      <th>SCHG</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SCIENGP</th>\n",
       "      <th>SCIENGRLP</th>\n",
       "      <th>SEMP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WAGP</th>\n",
       "      <th>WKEXREL</th>\n",
       "      <th>WKL</th>\n",
       "      <th>WRK</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>6250</td>\n",
       "      <td>63000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4740</td>\n",
       "      <td>2000</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>4110</td>\n",
       "      <td>6000</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP  CIT  COW  ENG  ESR  FFSP  LANX  MAR  MULTG  NOC  OCCP  PINCP  POVPIP  \\\n",
       "0    35    1    1    0    6     0     2    1      0   -1  6250  63000      -1   \n",
       "1    25    1    0    0    6     0     2    5      0   -1     9      0      -1   \n",
       "2    21    5    2    0    1     0     2    5      0   -1  4740   2000      -1   \n",
       "3    49    1    1    0    6     0     2    3      0   -1   110      0      -1   \n",
       "4    18    1    1    0    6     0     2    5      0   -1  4110   6000      -1   \n",
       "\n",
       "   RAC1P  SCH  SCHG  SCHL  SCIENGP  SCIENGRLP  SEMP  SEX   WAGP  WKEXREL  WKL  \\\n",
       "0      1    1     0    17        0          0     0    1  63000        0    1   \n",
       "1      1    1     0    12        0          0     0    1      0        0    3   \n",
       "2      6    3    15    19        0          0     0    2   2000        0    1   \n",
       "3      1    1     0    21        1          2     0    1      0        0    2   \n",
       "4      1    2    15    16        0          0     0    2   6000        0    1   \n",
       "\n",
       "   WRK  STATE  \n",
       "0    0     18  \n",
       "1    0     48  \n",
       "2    0     18  \n",
       "3    0      4  \n",
       "4    2      6  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in Data\n",
    "\n",
    "df = pd.read_csv('./data/data_clean.csv', index_col=False)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGEP</th>\n",
       "      <th>CIT</th>\n",
       "      <th>COW</th>\n",
       "      <th>ENG</th>\n",
       "      <th>ESR</th>\n",
       "      <th>FFSP</th>\n",
       "      <th>LANX</th>\n",
       "      <th>MAR</th>\n",
       "      <th>MULTG</th>\n",
       "      <th>NOC</th>\n",
       "      <th>OCCP</th>\n",
       "      <th>PINCP</th>\n",
       "      <th>POVPIP</th>\n",
       "      <th>RAC1P</th>\n",
       "      <th>SCH</th>\n",
       "      <th>SCHG</th>\n",
       "      <th>SCHL</th>\n",
       "      <th>SCIENGP</th>\n",
       "      <th>SCIENGRLP</th>\n",
       "      <th>SEMP</th>\n",
       "      <th>SEX</th>\n",
       "      <th>WAGP</th>\n",
       "      <th>WKEXREL</th>\n",
       "      <th>WKL</th>\n",
       "      <th>WRK</th>\n",
       "      <th>STATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>27800</td>\n",
       "      <td>136</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>38000</td>\n",
       "      <td>442</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>410</td>\n",
       "      <td>89000</td>\n",
       "      <td>501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>96400</td>\n",
       "      <td>501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9700</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AGEP  CIT  COW  ENG  ESR  FFSP  LANX  MAR  MULTG  NOC  OCCP  PINCP  POVPIP  \\\n",
       "0    49    1    0    0    6     0     2    3      1    2     9  27800     136   \n",
       "1    70    1    0    0    6     0     2    2      1    0     9  38000     442   \n",
       "2    58    1    1    0    1     0     2    1      1    0   410  89000     501   \n",
       "3    76    1    0    0    6     0     2    2      1    0     9  96400     501   \n",
       "4    57    1    0    0    6     0     2    3      1    0     9   9700      73   \n",
       "\n",
       "   RAC1P  SCH  SCHG  SCHL  SCIENGP  SCIENGRLP  SEMP  SEX   WAGP  WKEXREL  WKL  \\\n",
       "0      1    1     0    19        0          0     0    1      0       12    3   \n",
       "1      1    1     0    13        0          0     0    1      0       12    3   \n",
       "2      1    1     0    16        0          0     0    1  89000        1    1   \n",
       "3      1    1     0    21        2          2     0    1      0        0    3   \n",
       "4      1    1     0    16        0          0     0    1      0        0    3   \n",
       "\n",
       "   WRK  STATE  \n",
       "0    2     26  \n",
       "1    2     42  \n",
       "2    1     36  \n",
       "3    2     17  \n",
       "4    2     39  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tried modeling, but fit would not work because I do not have enough memory. Tried Google Collab to run the fit, but it also did not have the memory to do the work.\n",
    "# Decided to take my 3 million + observations and reduce them down to a lower number using pandas random sample.\n",
    "# Returns a new object of same type as caller containing n items randomly sampled from the caller object. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html\n",
    "\n",
    "df = df.sample(n=10000, replace=False, axis=0, ignore_index=True, random_state=42)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the root mean squared error from y and y predictions of models\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y \n",
    "\n",
    "X = df.drop(columns=['SEX', 'FFSP', 'LANX', 'NOC', 'RAC1P', 'SCH', 'SCHG'])\n",
    "y = df['SEX']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "Xs_train = ss.fit_transform(X_train)\n",
    "Xs_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.5049\n",
       "1    0.4951\n",
       "Name: SEX, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the baseline accuracy (% of the majority class, Male)\n",
    "# benchmark to beat is .509706\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is my hypothesis for how to make the story work. Then how changes were made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "logreg_y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7036334272900912"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y, logreg_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "gnb_y_pred = gnb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120393247567159"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y, gnb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=2, random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "rf.fit(Xs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf_y_pred = rf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7066116330771806"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse(y, rf_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5116666666666667\n",
      "0.49333333333333335\n",
      "0.5013333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but GaussianNB was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, lr.predict(X_test)))\n",
    "print(accuracy_score(y_test, gnb.predict(X_test)))\n",
    "print(accuracy_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['AGEP'] > 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y \n",
    "\n",
    "X = df.drop(columns=['SEX', 'CIT', 'COW', 'ENG', 'FFSP', 'LANX', 'NOC', 'OCCP', 'POVPIP', 'SCH', 'SCIENGRLP', 'SCIENGRLP', 'SEMP', 'WKL', 'WKEXREL', 'STATE'])\n",
    "y = df['SEX']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempted to see what would happen if I used PolynomialFeatures instead of StandardScaler. Went back to StandardScaler because results of Polynomial Features were worse.<br>\n",
    "Accuracy score of 0.50628... for Polynomial Features vs. 0.528571... for StandardScaler\n",
    "\n",
    "- OneHotEncoder for 'CIT', 'COW', 'ENG', 'ESR', 'MAR', 'MULTG', 'SCHL', 'WKEXREL', 'WKL', 'WRK', 'STATE'\n",
    "- PolynomialFeatures for 'AGEP', 'PINCP', 'WAGP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column transformer, OneHotEncoder is used on the categorical columns to create the same categories in each feature so the array produced will be the same shape as the data used to train the model\n",
    "# col_trans = ColumnTransformer([\n",
    "#    ('ohe', OneHotEncoder(), ['CIT', 'COW', 'ENG', 'ESR', 'MAR', 'MULTG', 'SCHL', 'WKEXREL', 'WKL', 'WRK', 'STATE']),\n",
    "#    ('poly', PolynomialFeatures(), ['AGEP', 'PINCP', 'WAGP'])],\n",
    "#    remainder='passthrough',\n",
    "#    sparse_threshold=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping the StandardScaler to compare with the Multidimensional Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column transformer, OneHotEncoder is used on the categorical columns to create the same categories in each feature so the array produced will be the same shape as the data used to train the model\n",
    "col_trans = ColumnTransformer([\n",
    "    ('ohe', OneHotEncoder(), ['ESR', 'MAR', 'MULTG', 'RAC1P', 'WRK']),\n",
    "    ('ss', StandardScaler(), ['AGEP', 'PINCP', 'WAGP'])],\n",
    "    remainder='passthrough',\n",
    "    sparse_threshold=0)\n",
    "\n",
    "\n",
    "#removed 'SEMP' because of this error: ValueError: Found unknown categories [-7000, -380, 100, 220, 3800, 7200, 8800, 9100, 13700, 27000, 31000, 70000, 77000, 324000] in column 7 during transform\n",
    "# may need to go back to creation of X and y before train, test, split to narrow down the columns to just the ones in this cell???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline StandardScaler + instantiate Logistic Regression\n",
    "pipe_log = Pipeline([\n",
    "    ('col_trans', col_trans),\n",
    "    ('log_reg', LogisticRegression(solver='saga', random_state=42, n_jobs=3, max_iter=10000))\n",
    "])\n",
    "\n",
    "# solver :  Algorithm to use in the optimization problem. Default is 'lbfgs'. To choose a solver, you might want to consider the following aspects: 'sag' and 'saga' are faster for large data sets\n",
    "# increased max_iter to 1000, 2000, 3000, 5000, 10000 because of max iter errors\n",
    "# https://datascience.stackexchange.com/questions/91225/why-gridsearchcv-returns-nan\n",
    "\n",
    "# error saying l1_ratio needed to be between 0 and 1, so added l1_ratio=0.5 to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_log = {\n",
    "    'log_reg__penalty': ['l2'],\n",
    "    'log_reg__C': [0.0001, 0.001, .01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "log_gs = GridSearchCV(pipe_log, param_grid=params_log, scoring = scoring, refit='accuracy', error_score='raise', cv=5)\n",
    "\n",
    "# add scoring f1 because of  \"UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan\"\n",
    "# SCORING = ['accuracy', 'precision', 'recall', 'f1' ]  ---want to add\n",
    "# https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "             estimator=Pipeline(steps=[('col_trans',\n",
       "                                        ColumnTransformer(remainder='passthrough',\n",
       "                                                          sparse_threshold=0,\n",
       "                                                          transformers=[('ohe',\n",
       "                                                                         OneHotEncoder(),\n",
       "                                                                         ['ESR',\n",
       "                                                                          'MAR',\n",
       "                                                                          'MULTG',\n",
       "                                                                          'RAC1P',\n",
       "                                                                          'WRK']),\n",
       "                                                                        ('ss',\n",
       "                                                                         StandardScaler(),\n",
       "                                                                         ['AGEP',\n",
       "                                                                          'PINCP',\n",
       "                                                                          'WAGP'])])),\n",
       "                                       ('log_reg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           n_jobs=3,\n",
       "                                                           random_state=42,\n",
       "                                                           solver='saga'))]),\n",
       "             param_grid={'log_reg__C': [0.0001, 0.001, 0.01],\n",
       "                         'log_reg__penalty': ['l2']},\n",
       "             refit='accuracy',\n",
       "             scoring=['accuracy', 'precision', 'recall', 'f1'])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6255166217430368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'log_reg__C': 0.01, 'log_reg__penalty': 'l2'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(log_gs.best_score_)\n",
    "log_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.62785265049416, 0.6370494551550713)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_gs.score(X_train, y_train), log_gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1372\n",
       "1    1014\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(log_gs.predict(X_test)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y \n",
    "\n",
    "X = df.drop(columns=['SEX'])\n",
    "y = df['SEX']\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS with 3 components\n",
    "\n",
    "embedding = MDS(n_components=3, random_state=42)\n",
    "X_transformed_3 = embedding.fit_transform(X_train)\n",
    "X_test_transformed_3 = embedding.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds = LogisticRegression()\n",
    "lr_mds.fit(X_transformed_3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.703663880800242"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds_y_pred_3 = lr_mds.predict(X_test_transformed_3)\n",
    "rmse(y_train, lr_mds_y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2386, 7000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14988/1856849192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multilabel\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    333\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2386, 7000]"
     ]
    }
   ],
   "source": [
    "# accuracy_score(y_test, lr_mds.predict(X_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulled in StandardScaler to see if this helps the MDA\n",
    "ss = StandardScaler()\n",
    "Xs_train = ss.fit_transform(X_train)\n",
    "Xs_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS with 7 components and StandardScaler\n",
    "\n",
    "embedding = MDS(n_components=7, random_state=42)\n",
    "X_transformed = embedding.fit_transform(Xs_train[:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds = LogisticRegression()\n",
    "lr_mds.fit(X_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6561794179730331"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds_y_pred = lr_mds.predict(X_transformed)\n",
    "rmse(y_train, lr_mds_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying MDS with 4 components and StandardScaler, somewhat of a better RMSE score than MDS with 7 components.\n",
    "embedding = MDS(n_components=4, random_state=42)\n",
    "X_transformed = embedding.fit_transform(Xs_train[:7000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds = LogisticRegression()\n",
    "lr_mds.fit(X_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6572670690061994"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_mds_y_pred = lr_mds.predict(X_transformed)\n",
    "rmse(y_train, lr_mds_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logisitic Regression\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ConfusionMatrixDisplay.from_estimator(log_gs,\n",
    "                                      X_test,\n",
    "                                      y_test,\n",
    "                                      display_labels=['Male', 'Female'], \n",
    "                                      cmap='Reds', \n",
    "                                      ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MDS - Logisitic Regression\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, lr_mds_y_pred, display_labels=['Male', 'Female'], cmap='Reds', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.6357709543381819\n",
      "Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 11 features, but LogisticRegression is expecting 3 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14988/3055586169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Logistic Regression'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr_mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mVector\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \"\"\"\n\u001b[1;32m--> 425\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m             \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\linear_model\\_base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\hatch\\anaconda3\\envs\\dsi-222\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    401\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m                 \u001b[1;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: X has 11 features, but LogisticRegression is expecting 3 features as input."
     ]
    }
   ],
   "source": [
    "# ROC Accuracy score\n",
    "print('Logistic Regression')\n",
    "print(roc_auc_score(y_test, log_gs.predict(X_test)))\n",
    "print('Logistic Regression')\n",
    "print(roc_auc_score(y_test, lr_mds.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.5285714285714286\n",
      "{'log_reg__C': 0.001, 'log_reg__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression')\n",
    "print(log_gs.best_score_)\n",
    "print(log_gs.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.6370494551550713\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "print('Logistic Regression')\n",
    "print(accuracy_score(y_test, log_gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.650887573964497\n"
     ]
    }
   ],
   "source": [
    "# Precision score\n",
    "print('Logistic Regression')\n",
    "print(precision_score(y_test, log_gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.5631399317406144\n"
     ]
    }
   ],
   "source": [
    "# Recall score\n",
    "print('Logistic Regression')\n",
    "print(recall_score(y_test, log_gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.5631399317406144\n"
     ]
    }
   ],
   "source": [
    "# Specificity score\n",
    "print('Logistic Regression')\n",
    "print(recall_score(y_test, log_gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "0.6038426349496798\n"
     ]
    }
   ],
   "source": [
    "# F1 score\n",
    "print('Logistic Regression')\n",
    "print(f1_score(y_test, log_gs.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5088948787061995 1.5088013411567478\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean(), y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5088013411567478"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.sum()/y_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.508801\n",
       "1    0.491199\n",
       "Name: SEX, dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Problem, Conclusions, and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Answer based on the following conclusions:__ Overall, the Logistic Regression model was the best model to identify Service Dogs vs Dog Training Subreddits. The SVM: C-Support Vector Classification model could also be used since it was very close in a lot of ways also. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Model                                | Best Score (gridsearch) | Train Score       | Test Score        | Accuracy          | ROC Accuracy      | Precision         | Recall            | Specificity       | F1 Score          |\n",
    "|----------|--------------------------------------|-------------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n",
    "| gs       | Random Forest Classifier             | 0.952763058289175       | 1.000000000000000 | 0.953020134228187 | 0.953020134228187 | 0.953012220770418 | 0.946453407510431 | 0.960479887085391 | 0.945544554455445 | 0.953415061295972 |\n",
    "| tree_gs  | Decision Tree Classifier             | 0.953217259651779       | 1.000000000000000 | 0.949134581419993 | 0.949134581419993 | 0.949132278385616 | 0.947294448348559 | 0.951305575158786 | 0.946958981612447 | 0.949295774647887 |\n",
    "| log_gs   | Logistic Regression                  | 0.965480696442089       | 0.999848599545798 | 0.967855881314023 | 0.967855881314023 | 0.967844989963256 | 0.958506224066390 | 0.978122794636556 | 0.957567185289957 | 0.968215158924205 |\n",
    "| gs_svm   | SVM: Linear SVC                      | 0.960938682816048       | 1.000000000000000 | 0.962910632285411 | 0.962910632285411 | 0.962901981296022 | 0.955555555555555 | 0.971065631616090 | 0.954738330975954 | 0.963248162408120 |\n",
    "| gs_svc   | SVM: C-Support Vector Classification | 0.899621498864496       | 0.933080999242997 | 0.914517838219710 | 0.914517838219710 | 0.914575387370373 | 0.965162311955661 | 0.860268172194777 | 0.968882602545968 | 0.909701492537313 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the Train Score and Test Score of the SVM: C-Support Vector Classification \n",
    "0.933080999242997 - 0.914517838219710"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between the Train Score and Test Score of the Logistic Regression model\n",
    "0.999848599545798 - 0.967855881314023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "\n",
    "__Bias/Variance Tradeoff__ All of the models were overfit where the train score was higher than the test score. The *Logistic Regression* model was the least overfit and performed the best in the tradeoff because it had the highest train/test scores. The *SVM: C-Support Vector Classification* was a close second because the difference between the train and test scores was the lowest. <br>  \n",
    "\n",
    "__Accuracy__ Number of correct predictions divided by the total number of predications, multiplied by 100 to get percentage: *Logistic Regression* had the best accuracy because it predicted the correct Subreddit 96.7% of the time. This is a reliable indicator because of the basically even class distribution.<br>\n",
    "\n",
    "__Precision__ Number of true positives divided by the number of true positives plus the false positives: *SVM: C-Support Vector Classification* had the best precision with 96.1% precisely predicted the correct Subreddit. This is not the best indicator because of the even class distribution. Precision in this situation is showing us how many submissions that were predicted as being correct and actually were correct.<br>\n",
    "\n",
    "__Recall__ Fraction of samples from a class which are correctly predicted by the model: *Logistic Regression* had the best recall because of the correctly labeled Subreddits, the model predicted 97.8% correct. <br>\n",
    "\n",
    "__Specificity__ Fraction of samples from a class which are incorrectly predicted as incorrect (True negative rate): *SVM: C-Support Vector Classification* had the best recall because of the incorrectly labeled Subreddits, the model predicted those correctly incorrect at 96.8%. This is not a reliable indicator in this specific situation. <br>\n",
    "\n",
    "__F1 Score__ Combination of precision and recall to check on the balance of the two: *Logistic Regression* has the highest F1 Score, so therefore, is the most balanced of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Roc Curve for Logistic Regression\n",
    "plot_roc_curve(log_gs, X_test, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Roc Curve for Logistic Regression\n",
    "plot_roc_curve(gs_svc, X_test, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendations\n",
    "\n",
    "__Implications__ Even though the Logistic Regression model was the most accurate at predicting the difference between the Service Dogs and Dog Training Subreddits, the classifications offered little to help predict training opportunities for service and other dogs. Classification opportunities could be to look more into the most common words and compare the differences between each Subreddit. <br>\n",
    "\n",
    "__Next Steps__ Specifically looking for training theories and pedagogy related words could be a valuable opportunity in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/20-popular-machine-learning-metrics-part-1-classification-regression-evaluation-metrics-1ca3e282a2ce"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec913dcbef39c008784dc6fe095f00003c17434557640291833f1bcf12636147"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('dsi-222')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
